---
title: "[お役立ち]コミュニケーションの計算論モデルをGithubで再現"
date: "2021-12-08T00:00:00"
draft: True

categories: ["お役立ち"]
cover: "/img/image_PC.png"
tags: ["研究", "お役立ち"]
summary: "理性的言語行為モデルを再現しよう！"
---
<script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<script language="JavaScript">
$(document).ready( function () {
   $("a[href^='http']:not([href*='" + location.hostname + "'])").attr('target', '_blank');
})
</script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [["\\(","\\)"] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>


本記事は、[Open and Reproducible Scienceアドカレ2021](https://adventar.org/calendars/6267)の8日目の記事です。昨年は[Githubをつかえ](https://susishushi.github.io/my_blog/post/gh/)というアクの強い記事を書いたので、今年もGithubに関する記事を書きます。<br>
<br>

## なにをするのか
最近読んでおもしろかった[コミュニケーションの計算論モデルに関する論文 (Bohn et al., 2021, PsyArXiv)](https://psyarxiv.com/yd9k4)がGithubで情報を公開しているので、それを再現しましょう。<br>
<br>

## Bohn et al. 2021はどういう論文？

 -  人の言語コミュニケーションに適用される理性的言語行為フレームワーク (Rational Speech Act framework: RSA) を類人猿のコミュニケーションに適用しよう。<br>
 -  推論モデルの予測がチンパンジーの行動データと合致。<br>
 -  チンパンジーの (推論の結果として生じる) 接近・回避行動は、対象個体との関係性と関連が高く、次いで表情と関連している (モダリティ間の相対的重要性から複数のモダリティを対象とするコミュニケ―ションの検討が必要であることを指摘)。

<br>
大体こんな感じ。詳細は読んでください。<br>
<br>

## 計算論モデルの詳細
ここでは対象論文で用いる数式を簡単に紹介しておきます。面倒な人は一気に読み飛ばしてください (ここは自分用のメモに近いです)。まず、ある特定の$発話u$が所与の時にその発話者が$意図i$を持っている確率を$P(意図i | 発話u)$とします。これは以下のように定義されます：<br>

$$P(意図i | 発話u) \propto P(発話u | 意図i)P(意図i)$$

このとき$P(意図i | 発話u)$は、意図iが所与のときの$発話u$が生じる尤度$(P(発話u | 意図i)$, チンパンジーの例：親和意図を持った時に腕を上げる) とそもそも$意図i$を持つことに関する事前確率 ($P(意図i)$, 例：親和意図を持つ) に分解できます。<br>
<br>
今回の例は類人猿のコミュニケーションが対象なので、$発話u$は$ジェスチャーg$と$表情f$に分解して以下のように尤度を定義します：<br>

$$P(発話u | 意図i) = P(ジェスチャーg, 表情f | 意図i) = \mathcal{L}(ジェスチャーg, 意図i | \theta_{g})\mathcal{L}(表情f, 意図i | \theta_{f})$$


ある信号 $(g, f)$ とある意図 ($i$) の意味空間に関する確率的マッピングを$\theta$としてここでは表現しています。$\mathcal{L}(表情f, 意図i | \theta_{f})$は「$表情f$の中でも$意図i$と確率的に対応する$表情f$の表現なんだなぁ」くらいに理解しといてください。少なくとも僕の理解度はその程度です。<br>

さて、発話は意図の事前確率 ($P(意図i)$) によって文脈化されるので、これは個体間の$社会関係s$とその$発話文脈c$の関数として考えることができます：<br>

$$P(意図i) = P(意図i | 文脈c, 社会関係s) = \rho_c \rho_s$$

このとき$\rho$は文脈および社会的関係の方向性と強度に関するパラメータとします。
<br><br>

## 観測データの詳細 [Oña et al., 2019](https://peerj.com/articles/7623/)
次に推論モデルによる予測と比較するための観測データに関する詳細は以下の通りです。<br>
・対象：半野生のチンパンジー72人<br>
・データ：信号とその受け手による反応 (人によるCoding)。<br>
・ジェスチャー：腕を伸ばす・曲げる ($\theta_g$)<br>
・表情：真顔・歯むき出し・とんがり唇 ($\theta_f$)<br>
・文脈：ポジティブ・ネガティブ ($\rho_c$)<br>
・送り手との関係性：支配・従属 ($\rho_s$)<br>
・相互作用の結果：接近・回避 (受け手による意図iの解釈と想定)<br>
<br>
数理的表現の設定 (先行研究と経験に基づいてるけど恣意的な設定であることは筆者も認めてる) <br>
・目的変数 (相互作用の結果)：0-1の値 ($0-0.5 = 接近；0.5-1 = 回避$)<br>
・ジェスチャー：腕を伸ばす = 少しネガティブ ($\theta_gs = 0.53$); 腕を曲げる = 少しポジティブ ($\theta_gb = 0.47$)<br>
・表情：真顔 = 完全中立 ($\theta_fn = 0.5$); 歯むき出し = 少しネガティブ ($\theta_fb = 0.6$); とんがり唇 = かなりネガティブ ($\theta_ff = 0.9$)<br>
・文脈：ネガティブ ($\rho_cn = 0.7$); ポジティブ ($\rho_cp = 0.3$)<br>
・関係性：支配 = ネガティブ ($\rho_sd = 0.25$); 従属 = ポジティブ ($\rho_ss = 0.75$)<br><br>

さて、素材がそろいました。これらのモデルによる予測と実データの対応を自分のPCで確認してみましょう。<br><br>

## GithubからあなたのPCへ
ここでは、[Bohn et al., 2021](https://psyarxiv.com/yd9k4)が用意してくれたGithubのリポジトリから情報を取得します。<br>
[昨年の記事](https://susishushi.github.io/my_blog/post/gh/)では、Githubからgitを使ってリポジトリを持ってくる解説をした（したのか？）けど、今回はWindowsのWSL2(いわゆる仮想環境でのUbuntu)でRstudio serverを立ち上げます。<br><br>
Windowsの人は、[関西学院大学柏原さんの記事](https://kscscr.com/archives/windows-wsl-ubuntu-cmdstan.html)を参考にしてWSL2を使ったRserverの環境を構築しなさい。<br><br>
「な、なぜそんなことを…。」と思われるかもしれませんが、このリポジトリでは[RwebPPL](https://github.com/mhtess/rwebppl)と呼ばれるパッケージをインストールする必要があり、このパッケージはUbuntuとMacしかサポートしてないからです (だからWindowsじゃなかったら[メキシコ流のやり方](https://susishushi.github.io/my_blog/post/gh/)でおっけーです)。ちなみに[Webppl](http://webppl.org/)はJavascriptに埋め込まれた機能豊富な確率的プログラミング言語です。<br><br>


Rstudio serverが立ち上がったら「File」→「New Project...」→「Version Control」→「Git」→「”Repository URL”に下画像のようにコピーした情報を入力して”Create Project”」で筆者が提供していた環境を丸々ローカルな環境で再現できます。<br>
  <img src="/img/rsa/rsa1.png" width=80%>




上手くいけば以下のような画面が出てます。<br>
  <img src="/img/rsa/rsa2.png" width=80%>



Filesウィンドウ内の「analysis」を開いて「model.rmd」を開いてみましょう。RmdとはRmarkdownファイルのことで、Rコードが埋め込まれたイカしたドキュメントファイルです。<br>
「model.rmd」を無事開けたら、まずはインストールされてないパッケージを適宜インストールしてください（rmdファイル開いたら、「おい、このパッケージがないぞ！」と言われるので適宜必要パッケージをインストールしてください）。ただし、[RwebPPL](https://github.com/mhtess/rwebppl)は、[node](https://nodejs.org/en/), [npm](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) などをターミナル上(sudoなど)でインストールしたうえで
```r:devtool_install
devtools::install_github("mhtess/rwebppl")
```
でインストールしなければならないので注意しましょう。<br>
<br>
<br>

## ふるえるぞハート！燃えつきるほどニット！
お疲れ様です。結果を再現しましょう。下の画像の位置にあるknitボタンを押しましょう (もしくはCtrl+Shift+K)。すると自動で全てのRコードが回って、htmlファイルを出力してくれるはずです。<br>
<br>

  <img src="/img/rsa/rsa3.png" width=80%>

<br>
最終的には以下のような画面が出るはずです。Congratulation！<br>
逐一読んでいけば、どういうデータを使ってどういうモデルを回しているかが理解できるはずです。ちょっと量がありすぎるのでこの記事では逐一解説はしませんが…。<br><br>
  <img src="/img/rsa/rsa4.png" width=80%>

<br>

## …で、結果はどうなのよ？
model.rmd：Lines 166~247が全体モデルとしての結果と対応したコードになります。結果としては、下図の通り今回用いたパラメータ設定によるモデルの予測は観測データとかなり似た結果が得られたようです(y軸がモデルの予測、x軸が実際のデータ)。<br>
  <img src="/img/rsa/rsa5.png" width=80%>
  
<br>
表情、ジェスチャー、文脈、関係性のみ (model.rmd：250-325 linesでパラメータの設定) でモデルを作成した場合の結果が以下の通りです。<br>
<br>
  <img src="/img/rsa/rsa6.png" width=80%>

<br>
図の通り、関係性がとても大事だよ。表情も回避行動の予測ができなくはないよ（パラメータ設定段階で自明ですが…）ということがわかります。<br><br>
このRmdファイルをニットするまでは「よーし、パパ、他のパラメータ設定も試してみちゃうぞ～」と思ってたんですが筆者が事前に別のパラメータ設定（ジェスチャーと表情の重要性を反転させた設定：ジェスチャーの意味の強度を増やし、表情の重要度を減らす）を試した結果もちゃんと確認してました。<br><br>
その結果、データとモデルの予測はいまいちであったことも報告されており、ジェスチャーの相対的重要度はいまいちなのが現実を反映していそう、というインプリケーションも得られたっぽいです。<br>
<br>
  <img src="/img/rsa/rsa7.png" width=80%>

<br>

## 感想：Open Scienceの大変さ
WSL2を導入する必要性の話からも分かる通り、ある論文の解析結果を再現する、というのは結構大変な作業であることがままあります (恥ずかしながらWebPPLという言語があることもこれまで知りませんでしたし…)。<br><br>
現在の査読案件でも、本文内に「データと解析はOSFにアップロードしているので結果を再現できますよ！」と書かれていたので、解析の詳細を確認するためにアップロードされたRmdファイルを開いた結果、必要なデータファイルがアップロードされてなかった…なんてこととかも経験しました（採択・出版された論文でもこういう事例がないこともない、いやそれは筆者に連絡しろよって話なんですけど）。<br><br>
いろいろと仕組みが必要だなぁと感じたので、イイ感じの情報があればまた共有します。みんなも耳寄り情報はオープンにしてくれよな！<br>
ほな！
<br>

<br>
[<img src="/img/violet-button.png" width=20%>](https://www.buymeacoffee.com/nsushishushi)
<br>

##### 他にこんな記事があるよ
 [CODEとトリマでポイ活](https://susishushi.github.io/my_blog/post/poikatsu/)<br>
[表情刺激のデータベース(fIMDb)](https://susishushi.github.io/my_blog/post/fimdb/)
<br>
[OpenFaceで表情分析](https://susishushi.github.io/my_blog/post/of/)
